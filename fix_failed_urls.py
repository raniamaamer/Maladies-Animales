import pandas as pd
import requests
from tqdm import tqdm
import time
import sys

# Configuration des fichiers
OUTPUT_CSV = "data/output/output_dataset.csv"
INPUT_CSV = "data/input/urls.csv"
RETRY_FILE = "data/input/urls_to_retry.csv"
EXPANDED_FILE = "data/input/urls_expanded_retry.csv"

def identify_failed_urls():
    """Identifie les URLs qui ont √©chou√© lors du scraping"""
    
    print("="*70)
    print("üîç √âTAPE 1: IDENTIFICATION DES URLs PROBL√âMATIQUES")
    print("="*70)
    print()
    
    try:
        df = pd.read_csv(OUTPUT_CSV, encoding='utf-8')
        print(f"‚úì {len(df)} r√©sultats charg√©s")
        print()
        
        # Crit√®res d'√©chec
        failed_urls = []
        
        # 1. Contenu trop court (< 100 mots)
        short_content = df[df['nb_mots'] < 100].copy()
        short_content['raison'] = 'Contenu trop court'
        failed_urls.append(short_content)
        
        # 2. Pages Cloudflare
        cloudflare = df[df['contenu'].str.contains('Cloudflare|Ray ID|v√©rifier la s√©curit√©', case=False, na=False)].copy()
        cloudflare['raison'] = 'Page Cloudflare'
        failed_urls.append(cloudflare)
        
        # 3. Erreurs de scraping
        errors = df[df['titre'].str.contains('Erreur|non trouv√©', case=False, na=False)].copy()
        errors['raison'] = 'Erreur de scraping'
        failed_urls.append(errors)
        
        # 4. Contenu "inconnu" partout
        unknown = df[
            (df['date_publication'] == 'inconnue') & 
            (df['lieu'] == 'inconnu') & 
            (df['maladie'] == 'inconnue') &
            (df['nb_mots'] < 200)
        ].copy()
        unknown['raison'] = 'Donn√©es incompl√®tes'
        failed_urls.append(unknown)
        
        # Combiner et supprimer les doublons
        all_failed = pd.concat(failed_urls, ignore_index=True)
        all_failed = all_failed.drop_duplicates(subset=['code'])
        
        print("üìä STATISTIQUES DES √âCHECS")
        print("-"*70)
        print(f"Contenu trop court : {len(short_content)}")
        print(f"Pages Cloudflare : {len(cloudflare)}")
        print(f"Erreurs de scraping : {len(errors)}")
        print(f"Donn√©es incompl√®tes : {len(unknown)}")
        print()
        print(f"üîÑ Total URLs √† re-scraper : {len(all_failed)}/{len(df)}")
        print()
        
        if len(all_failed) > 0:
            # Afficher quelques exemples
            print("üìù EXEMPLES D'URLs PROBL√âMATIQUES")
            print("-"*70)
            for idx, row in all_failed.head(10).iterrows():
                print(f"{row['code']}: {row['url']}")
                print(f"  ‚Üí Raison: {row['raison']} ({row['nb_mots']} mots)")
            print()
            
            # Sauvegarder la liste
            retry_df = all_failed[['code', 'url', 'raison', 'nb_mots']].copy()
            retry_df.to_csv(RETRY_FILE, index=False, encoding='utf-8')
            
            print(f"üíæ Liste sauvegard√©e : {RETRY_FILE}")
            print()
            
            return all_failed
        else:
            print("‚úÖ Aucune URL probl√©matique d√©tect√©e !")
            return None
            
    except FileNotFoundError:
        print(f"‚ùå Fichier introuvable : {OUTPUT_CSV}")
        return None
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        import traceback
        traceback.print_exc()
        return None

def expand_url(short_url, timeout=10):
    """D√©veloppe une URL raccourcie en suivant les redirections"""
    try:
        # Utiliser HEAD pour suivre les redirections
        response = requests.head(
            short_url, 
            allow_redirects=True, 
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
        )
        final_url = response.url
        
        # Si HEAD ne fonctionne pas, essayer GET
        if final_url == short_url or any(short in final_url for short in ['lc.cx', 'bit.ly']):
            response = requests.get(
                short_url,
                allow_redirects=True,
                timeout=timeout,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            )
            final_url = response.url
        
        return final_url
        
    except requests.exceptions.Timeout:
        return short_url
    except requests.exceptions.RequestException:
        return short_url
    except Exception:
        return short_url

def expand_failed_urls(failed_df):
    """D√©veloppe les URLs raccourcies parmi les URLs √©chou√©es"""
    
    print("="*70)
    print("üîó √âTAPE 2: D√âVELOPPEMENT DES URLs RACCOURCIES")
    print("="*70)
    print()
    
    if failed_df is None or len(failed_df) == 0:
        print("‚ö†Ô∏è Aucune URL √† traiter")
        return None
    
    # Identifier les URLs raccourcies
    short_patterns = ['lc.cx', 'bit.ly', 'tinyurl', 't.co', 'goo.gl']
    is_short = failed_df['url'].astype(str).apply(
        lambda x: any(pattern in x.lower() for pattern in short_patterns)
    )
    
    short_urls = failed_df[is_short]
    
    print(f"üîç URLs raccourcies d√©tect√©es : {len(short_urls)}/{len(failed_df)}")
    
    if len(short_urls) == 0:
        print("‚úì Aucune URL raccourcie √† d√©velopper")
        print()
        # Sauvegarder quand m√™me pour le re-scraping
        failed_df[['code', 'url', 'raison']].to_csv(EXPANDED_FILE, index=False, encoding='utf-8')
        print(f"üíæ Fichier sauvegard√© : {EXPANDED_FILE}")
        return failed_df
    
    print()
    print("üîÑ D√©veloppement en cours...")
    print("-"*70)
    
    expanded_urls = []
    changed_count = 0
    
    for idx, row in tqdm(failed_df.iterrows(), total=len(failed_df), desc="Progression"):
        original_url = str(row['url']).strip()
        
        # D√©velopper seulement si raccourcie
        if any(pattern in original_url.lower() for pattern in short_patterns):
            expanded = expand_url(original_url)
            
            if expanded != original_url:
                changed_count += 1
                if changed_count <= 5:
                    print(f"\n  ‚úì {row['code']}: {original_url}")
                    print(f"    ‚Üí {expanded}")
            
            expanded_urls.append(expanded)
        else:
            expanded_urls.append(original_url)
        
        time.sleep(0.3)  # Pause pour √©viter la surcharge
    
    print()
    print("-"*70)
    
    # Cr√©er le DataFrame final
    result_df = failed_df.copy()
    result_df['url_originale'] = result_df['url']
    result_df['url'] = expanded_urls
    
    # Sauvegarder
    output_df = result_df[['code', 'url', 'url_originale', 'raison']].copy()
    output_df.to_csv(EXPANDED_FILE, index=False, encoding='utf-8')
    
    print()
    print("="*70)
    print("‚úÖ D√âVELOPPEMENT TERMIN√â")
    print("="*70)
    print(f"üìä Statistiques :")
    print(f"   ‚Ä¢ Total URLs : {len(failed_df)}")
    print(f"   ‚Ä¢ URLs d√©velopp√©es : {changed_count}")
    print(f"   ‚Ä¢ URLs non modifi√©es : {len(failed_df) - changed_count}")
    print()
    print(f"üíæ Fichier sauvegard√© : {EXPANDED_FILE}")
    print()
    
    return result_df

def generate_report(failed_df):
    """G√©n√®re un rapport d√©taill√©"""
    
    print("="*70)
    print("üìã √âTAPE 3: RAPPORT FINAL")
    print("="*70)
    print()
    
    if failed_df is None or len(failed_df) == 0:
        print("‚úÖ Aucun probl√®me d√©tect√© !")
        return
    
    # Statistiques par raison
    print("üìä R√âPARTITION PAR TYPE D'√âCHEC")
    print("-"*70)
    for raison, group in failed_df.groupby('raison'):
        count = len(group)
        pct = (count / len(failed_df)) * 100
        print(f"{raison:.<40} {count:>4} ({pct:>5.1f}%)")
    print()
    
    # URLs Cloudflare
    cloudflare = failed_df[failed_df['raison'] == 'Page Cloudflare']
    if len(cloudflare) > 0:
        print("üõ°Ô∏è URLs BLOQU√âES PAR CLOUDFLARE")
        print("-"*70)
        print("Ces URLs sont difficiles √† scraper automatiquement.")
        print("Options :")
        print("  1. Utiliser undetected-chromedriver (d√©j√† propos√©)")
        print("  2. Scraping manuel")
        print("  3. Accepter la perte de ces donn√©es")
        print()
    
    # URLs avec contenu court
    short = failed_df[failed_df['raison'] == 'Contenu trop court']
    if len(short) > 0:
        print("üìù URLs AVEC CONTENU COURT")
        print("-"*70)
        print("Ces URLs peuvent n√©cessiter :")
        print("  1. Un temps d'attente plus long (JavaScript)")
        print("  2. Des s√©lecteurs CSS diff√©rents")
        print("  3. V√©rification manuelle de la validit√© de l'URL")
        print()
    
    # Prochaines √©tapes
    print("="*70)
    print("üöÄ PROCHAINES √âTAPES")
    print("="*70)
    print()
    print("1Ô∏è‚É£  OPTION 1 : Re-scraper avec le fichier g√©n√©r√©")
    print("   Modifiez main.py :")
    print(f"   INPUT_FILE = \"{EXPANDED_FILE}\"")
    print("   Puis lancez : python main.py")
    print()
    print("2Ô∏è‚É£  OPTION 2 : Installer undetected-chromedriver")
    print("   pip install undetected-chromedriver")
    print("   Puis utilisez le scraper am√©lior√©")
    print()
    print("3Ô∏è‚É£  OPTION 3 : V√©rification manuelle")
    print(f"   Ouvrez {EXPANDED_FILE} et v√©rifiez les URLs")
    print()
    print("="*70)

def main():
    """Fonction principale"""
    print("="*70)
    print("üîß OUTIL DE CORRECTION DES URLs √âCHOU√âES")
    print("="*70)
    print()
    
    # √âtape 1 : Identifier les URLs probl√©matiques
    failed_df = identify_failed_urls()
    
    if failed_df is None or len(failed_df) == 0:
        print("\n‚úÖ Aucune URL probl√©matique d√©tect√©e !")
        print("Votre scraping semble avoir bien fonctionn√©.")
        return
    
    print()
    
    # √âtape 2 : D√©velopper les URLs raccourcies
    expanded_df = expand_failed_urls(failed_df)
    
    print()
    
    # √âtape 3 : G√©n√©rer le rapport
    generate_report(expanded_df)

if __name__ == "__main__":
    main()