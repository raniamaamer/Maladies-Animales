# üêæ Projet Web Scraping - Maladies Animales

Syst√®me automatis√© d'extraction et d'analyse d'articles sur les maladies animales avec Selenium, ScrapingBee et LLM.

## üìã Description

Ce projet extrait automatiquement des informations √† partir d'URLs d'articles sur les maladies animales et g√©n√®re un dataset CSV structur√© avec :
- M√©tadonn√©es (titre, langue, dates, lieux)
- Analyse de contenu (maladie, animal concern√©)
- R√©sum√©s automatiques (50, 100, 150 mots)
- Gestion des sites prot√©g√©s (Cloudflare, WAHIS, etc.)

## üèóÔ∏è Architecture

```
animal_disease_scraper/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/           # Fichier URLs d'entr√©e (urls.csv)
‚îÇ   ‚îú‚îÄ‚îÄ output/          # R√©sultats (output_dataset.csv)
‚îÇ   ‚îî‚îÄ‚îÄ logs/            # Logs d'ex√©cution (scraping.log)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py       # Module Selenium + ScrapingBee
‚îÇ   ‚îú‚îÄ‚îÄ llm_processor.py # Module LLM (extraction m√©tadonn√©es r√©sum√©s)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py         # Utilitaires (nettoyage, d√©tection langue)
‚îÇ         
‚îî‚îÄ‚îÄ main.py              # Script principal
```

## üöÄ Installation

### 1. Pr√©requis
```bash
# Python 3.8+
python --version

# Chrome/Chromium install√© sur votre syst√®me
google-chrome --version  # ou chromium --version
```

### 2. Installation des d√©pendances
```bash
pip install -r requirements.txt
```

**D√©pendances principales :**
```txt
selenium==4.15.2
webdriver-manager==4.0.1
beautifulsoup4==4.12.2
lxml==4.9.3
pandas==2.1.3
numpy==1.26.2
langdetect==1.0.9
ollama==0.1.6
requests==2.31.0
tenacity==8.2.3

```

### 3. Installation d'Ollama (LLM local gratuit)

**Windows:**
- T√©l√©charger depuis https://ollama.com/download
- Installer et ex√©cuter
- Ouvrir terminal: `ollama pull llama3.2`

**V√©rifier l'installation:**
```bash
ollama list
# Doit afficher : llama3.2:latest
```

### 4. Configuration ScrapingBee (pour sites prot√©g√©s)

**‚ö†Ô∏è IMPORTANT : Cl√© API √† configurer**

1. Cr√©er un compte sur https://www.scrapingbee.com (1000 cr√©dits gratuits)
2. R√©cup√©rer votre API key
3. Dans `src/scraper.py`, ligne 8, remplacer :
```python
SCRAPINGBEE_API_KEY = "VOTRE_CLE_API_ICI"
```

**üîí S√©curit√© :** Apr√®s vos tests, **r√©voquez cette cl√©** depuis le dashboard ScrapingBee.

## üìä Pr√©paration des Donn√©es

### Format du fichier d'entr√©e

Cr√©ez `data/input/urls.csv` avec le format suivant :

```csv
code,url
code151 https://lc.cx/nKVbsM
code152 https://lc.cx/sXWRhi
code153 https://lc.cx/JSB3wp
```

**Colonnes obligatoires:**
- `code` : Identifiant unique (alphanum√©rique)
- `url` : URL de l'article

**Formats accept√©s :**
- D√©limiteurs : `,` ou `;` ou `\t` (auto-d√©tect√©)
- Encodage : UTF-8

## üéØ Utilisation

### Lancement du scraping complet

```bash
python main.py
```

Le script va automatiquement :
1. ‚úÖ Charger le fichier `data/input/urls.csv`
2. üîç Scraper chaque URL (avec gestion des sites prot√©g√©s)
3. üßπ Nettoyer et analyser le contenu
4. ü§ñ Extraire les m√©tadonn√©es avec le LLM
5. üíæ Sauvegarder les r√©sultats dans `data/output/output_dataset.csv`

### Suivi en temps r√©el

```bash
# Dans un autre terminal
tail -f data/logs/scraping.log
```

### Configuration de la vitesse LLM

Dans `main.py`, ligne 10 :

```python
LLM_MODE = "fast"   # ‚ö° Rapide : ~10 sec/article (recommand√©)
# ou
LLM_MODE = "normal" # üéØ Pr√©cis : ~30 sec/article
```

## üìä Flux d'Ex√©cution

```
main.py
   ‚îÇ
   ‚îú‚îÄ‚Üí 1. Chargement CSV (auto-d√©tection d√©limiteur)
   ‚îÇ      ‚îî‚îÄ‚Üí D√©tection colonnes code/url
   ‚îÇ
   ‚îú‚îÄ‚Üí 2. Initialisation Selenium
   ‚îÇ
   ‚îú‚îÄ‚Üí 3. Pour chaque URL :
   ‚îÇ      ‚îÇ
   ‚îÇ      ‚îú‚îÄ‚Üí D√©tection site prot√©g√© ?
   ‚îÇ      ‚îÇ   ‚îú‚îÄ‚Üí OUI ‚Üí ScrapingBee (API)
   ‚îÇ      ‚îÇ   ‚îî‚îÄ‚Üí NON ‚Üí Selenium direct
   ‚îÇ      ‚îÇ
   ‚îÇ      ‚îú‚îÄ‚Üí Extraction contenu (titre + texte)
   ‚îÇ      ‚îÇ
   ‚îÇ      ‚îú‚îÄ‚Üí Validation contenu
   ‚îÇ      ‚îÇ   ‚îî‚îÄ‚Üí Si √©chec ‚Üí 2√®me tentative
   ‚îÇ      ‚îÇ
   ‚îÇ      ‚îú‚îÄ‚Üí Nettoyage texte (utils.py)
   ‚îÇ      ‚îÇ
   ‚îÇ      ‚îú‚îÄ‚Üí D√©tection langue
   ‚îÇ      ‚îÇ
   ‚îÇ      ‚îú‚îÄ‚Üí Analyse LLM :
   ‚îÇ      ‚îÇ   ‚îú‚îÄ‚Üí Extraction m√©tadonn√©es (date, lieu, maladie, animal)
   ‚îÇ      ‚îÇ   ‚îî‚îÄ‚Üí G√©n√©ration r√©sum√©s (50/100/150 mots)
   ‚îÇ      ‚îÇ
   ‚îÇ      ‚îî‚îÄ‚Üí Sauvegarde ligne dans CSV
   ‚îÇ
   ‚îî‚îÄ‚Üí 4. Rapport final
```

## üìù Fichiers de Sortie

### `data/output/output_dataset.csv`

Dataset final avec toutes les colonnes :

| Colonne | Description | Exemple |
|---------|-------------|---------|
| `code` | Identifiant unique | code152 |
| `url` | URL source | https://... |
| `titre` | Titre de l'article | "Alerte grippe aviaire" |
| `contenu` | Texte complet nettoy√© | "Un foyer de grippe..." |
| `langue` | Langue d√©tect√©e | fran√ßais / arabic / english |
| `nb_caracteres` | Nombre de caract√®res | 2847 |
| `nb_mots` | Nombre de mots | 421 |
| `date_publication` | Date extraite | 15-03-2024 |
| `lieu` | Pays/r√©gion | France |
| `maladie` | Maladie identifi√©e | grippe aviaire |
| `animal` | Esp√®ce concern√©e | poulets |
| `source_publication` | Type de source | site officiel / presse |
| `resume_50_mots` | R√©sum√© court | ... |
| `resume_100_mots` | R√©sum√© moyen | ... |
| `resume_150_mots` | R√©sum√© d√©taill√© | ... |

## ‚öôÔ∏è Configuration Avanc√©e

### 1. Modifier les sites prot√©g√©s

Dans `src/scraper.py`, ligne 145 :

```python
PROTECTED_DOMAINS = [
    "wahis.woah.org",
    "alyaum.com",
    "elfagr.org"
]
```

### 2. Changer le mod√®le LLM

Dans `src/llm_processor.py`, ligne 5 :

```python
MODEL = "llama3.2"      # Recommand√© (√©quilibre vitesse/qualit√©)
```

### 3. Ajuster les timeouts Selenium

Dans `src/scraper.py`, ligne 161 :

```python
WebDriverWait(driver, 15).until(...)  # Changer 15 ‚Üí 20 pour sites lents
time.sleep(3)  # Augmenter √† 5 si n√©cessaire
```

### 4. Utiliser un proxy

Dans `src/scraper.py`, fonction `setup_driver()` :

```python
options.add_argument('--proxy-server=http://votre-proxy:port')
```

## üîß R√©solution de Probl√®mes

### ‚ùå Erreur : "ChromeDriver not found"

**Solution :**
```bash
pip install --upgrade webdriver-manager
```

Le script t√©l√©charge automatiquement ChromeDriver au premier lancement.

---

### ‚ùå Erreur : "Ollama connection refused"

**Solution :**
```bash
# D√©marrer Ollama
ollama serve

# Dans un autre terminal, v√©rifier
ollama list
ollama run llama3.2 "test"
```

---

### ‚ùå Contenu vide ou "Titre non trouv√©"

**Causes possibles :**

1. **Site prot√©g√© par Cloudflare/Captcha**
   - V√©rifiez que ScrapingBee est configur√©
   - Ajoutez le domaine dans `PROTECTED_DOMAINS`

2. **Site trop lent √† charger**
   - Augmentez le timeout dans `scraper.py` (ligne 161)
   - Augmentez `time.sleep(3)` ‚Üí `time.sleep(5)`

3. **Structure HTML non reconnue**
   - Testez en mode non-headless : `options.add_argument("--headless")` ‚Üí commentez cette ligne
   - V√©rifiez les s√©lecteurs CSS dans `scraper.py`

---

### ‚ùå Erreur ScrapingBee : "Incorrect API key"

**Solution :**
1. V√©rifiez votre cl√© sur https://www.scrapingbee.com/dashboard
2. V√©rifiez qu'il n'y a pas d'espaces avant/apr√®s la cl√©
3. V√©rifiez que vous avez encore des cr√©dits

---

### ‚ùå Langue non d√©tect√©e (affiche "inconnu")

**Solution :**
- Le texte doit contenir au moins 10 caract√®res
- Pour l'arabe, v√©rifiez l'encodage UTF-8 du fichier
- Installez la derni√®re version : `pip install --upgrade langdetect`

---

### ‚ùå LLM trop lent (> 1 minute par article)

**Solutions :**

1. **Activer le mode fast** (main.py ligne 10)
   ```python
   LLM_MODE = "fast"
   ```

2. **Utiliser un mod√®le plus petit**
   ```bash
   ollama pull llama3.2:1b
   ```
   Puis dans `llm_processor.py` :
   ```python
   MODEL = "llama3.2:1b"
   ```

3. **V√©rifier l'utilisation GPU**
   ```bash
   ollama ps  # Doit afficher GPU si disponible
   ```

4. **R√©duire le contexte** (llm_processor.py ligne 39)
   ```python
   text_sample = text[:1000]  # Au lieu de 1500
   ```

---

### ‚ùå CSV mal format√© ou caract√®res √©tranges

**Solution :**
```python
# Dans main.py, forcer l'encodage UTF-8
df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
```

## üé® Personnalisation

### Ajouter un nouveau champ LLM

**1. Modifier le prompt** dans `src/llm_processor.py` ligne 39 :

```python
prompt = f"""... extrais les informations :
- "date_publication"
- "lieu"
- "maladie"
- "animal"
- "nombre_cas"  # ‚Üê Nouveau champ
...
"""
```

**2. Ajouter le champ par d√©faut** ligne 62 :

```python
return {
    ...,
    "nombre_cas": data.get("nombre_cas", "inconnu")
}
```

**3. Mettre √† jour main.py** ligne 190 :

```python
final_row = {
    ...,
    "nombre_cas": llm_fields.get("nombre_cas", "inconnu")
}
```

### Changer le format de date

Dans `src/llm_processor.py`, prompt ligne 42 :

```python
- "date_publication" (format YYYY-MM-DD, "inconnue" si absente)
```

## üìä Exemples de R√©sultats

### Exemple 1 : Site standard (succ√®s)

**Input :**
```csv
code123,https://example.com/article-grippe-aviaire
```

**Output :**
```csv
code123,https://example.com/article-grippe-aviaire,"Foyer de grippe aviaire d√©tect√©","Un nouveau foyer...",fran√ßais,1847,273,12-03-2024,Bretagne,grippe aviaire,poulets,presse,"Un foyer de grippe aviaire a √©t√© d√©tect√© en Bretagne..."
```

---

### Exemple 2 : Site prot√©g√© WAHIS (avec ScrapingBee)

**Input :**
```csv
code171,https://wahis.woah.org/#/in-review/5294
```

**Logs :**
```
üîí Site prot√©g√© d√©tect√© : https://wahis.woah.org/#/in-review/5294
‚Üí Utilisation directe de ScrapingBee...
üì° Appel ScrapingBee pour : https://wahis.woah.org/#/in-review/5294
‚úÖ ScrapingBee : succ√®s
‚úì Contenu valide r√©cup√©r√©
```

---

### Exemple 3 : √âchec de scraping

**Output :**
```csv
code999,https://site-inaccessible.com,"√âchec du scraping","Le contenu n'a pas pu √™tre extrait",inconnu,0,0,inconnue,inconnu,inconnue,inconnu,inconnu,"Scraping √©chou√©"
```

## ü§ù Alternatives LLM

### 1. Ollama (Recommand√© - Gratuit)
‚úÖ Gratuit, local, pas de limite  
‚úÖ Multilingue excellent  
‚úÖ Pas besoin d'API key  
‚úÖ Fonctionne hors ligne  

**Mod√®les recommand√©s :**
- `llama3.2` : √âquilibre vitesse/qualit√© (par d√©faut)
- `llama3.2:1b` : Plus rapide, qualit√© correcte
- `llama3.1:8b` : Meilleure qualit√©, plus lent

---

## üìà Performance

| √âtape | Temps moyen | Notes |
|-------|-------------|-------|
| Scraping (Selenium) | 3-5 sec/URL | Sites standards |
| Scraping (ScrapingBee) | 5-8 sec/URL | Sites prot√©g√©s |
| Analyse LLM (fast) | 8-12 sec/article | Mode rapide |
| Analyse LLM (normal) | 25-35 sec/article | Mode pr√©cis |
| **Total (50 URLs, fast)** | **12-18 minutes** | Recommand√© |
| **Total (50 URLs, normal)** | **28-38 minutes** | Production |

**Facteurs d'impact :**
- Vitesse CPU/GPU
- Longueur des articles
- Sites prot√©g√©s (+ lent)
- Mod√®le LLM utilis√©

## üêõ Logs et Debugging

### Fichiers de logs

```bash
# Log principal
data/logs/scraping.log

# Voir en temps r√©el
tail -f data/logs/scraping.log

# Chercher les erreurs
grep "ERROR" data/logs/scraping.log
grep "‚ùå" data/logs/scraping.log
```

### Logs d√©taill√©s

Le script g√©n√®re des logs structur√©s :

```
============================================================
[6/50] Traitement [code156]
URL: https://www.elfagr.org/4789113
============================================================
üîí Site prot√©g√© d√©tect√© : https://www.elfagr.org/4789113
‚Üí Utilisation directe de ScrapingBee...
üì° Appel ScrapingBee pour : https://www.elfagr.org/4789113
‚úÖ ScrapingBee : succ√®s
‚úì Contenu valide r√©cup√©r√©
‚Üí Extraction LLM en cours (mode: fast)...
‚Üí Extraction des m√©tadonn√©es...
‚Üí M√©tadonn√©es extraites: grippe aviaire / poulets / √âgypte
‚Üí G√©n√©ration r√©sum√© 50 mots...
‚Üí G√©n√©ration r√©sum√© 100 mots...
‚Üí G√©n√©ration r√©sum√© 150 mots...
‚úÖ Trait√© avec succ√®s
   ‚Ä¢ Titre: ÿ™ŸÅÿ¥Ÿä ÿ£ŸÜŸÅŸÑŸàŸÜÿ≤ÿß ÿßŸÑÿ∑ŸäŸàÿ± ŸÅŸä ÿßŸÑŸÇÿßŸáÿ±ÿ©
   ‚Ä¢ Mots: 486
   ‚Ä¢ Langue: arabic
üíæ Sauvegarde interm√©diaire (6 r√©sultats)
```

### Mode debug (voir le navigateur)

Dans `src/scraper.py`, ligne 13, **commenter** :

```python
# options.add_argument("--headless")  # ‚Üê D√©sactiv√©
```

Le navigateur s'ouvrira et vous verrez le scraping en direct.

## üí° Conseils et Bonnes Pratiques

### üéØ Avant de lancer sur 50 URLs

1. **Testez sur 3-5 URLs d'abord**
   ```csv
   code,url
   test1,https://example1.com
   test2,https://example2.com
   ```

2. **V√©rifiez la qualit√© du scraping**
   - Ouvrez `output.csv`
   - V√©rifiez que les titres sont corrects
   - V√©rifiez la longueur du contenu (> 100 mots)

3. **Validez les r√©sum√©s LLM**
   - Lisez quelques r√©sum√©s
   - V√©rifiez qu'ils sont en fran√ßais
   - V√©rifiez la coh√©rence

### ‚ö° Optimiser les performances

1. **Utiliser le mode fast** (ligne 10 main.py)
2. **Traiter par batch** : 10-15 URLs √† la fois
3. **√âviter les heures de pointe** pour ScrapingBee
4. **Fermer les autres applications** qui consomment RAM/CPU

### üîí S√©curit√© et √âthique

1. **Respectez les robots.txt**
   ```bash
   curl https://example.com/robots.txt
   ```

2. **Ajoutez des d√©lais** entre requ√™tes (d√©j√† fait : 2 sec)

3. **Ne partagez jamais vos cl√©s API**
   - ‚úÖ Utilisez `.env`
   - ‚ùå Ne commitez pas les cl√©s dans Git

4. **R√©voquez les cl√©s de test**
   - ScrapingBee : https://www.scrapingbee.com/dashboard

### üìä G√©rer de gros volumes

Pour > 100 URLs :

1. **Divisez le fichier** en plusieurs CSV
2. **Lancez en parall√®le** (avec prudence)
3. **Surveillez les cr√©dits** ScrapingBee
4. **Sauvegardez r√©guli√®rement** (d√©j√† fait : tous les 3 articles)

## üìö Ressources et Documentation

### D√©pendances Principales

- **Selenium** : https://selenium-python.readthedocs.io
- **BeautifulSoup4** : https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **Ollama** : https://ollama.ai/docs
- **ScrapingBee** : https://www.scrapingbee.com/documentation
- **Pandas** : https://pandas.pydata.org/docs/

### Tutoriels

- Selenium : https://realpython.com/modern-web-automation-with-python-and-selenium/
- Web Scraping √©thique : https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/
- Ollama guides : https://github.com/ollama/ollama/tree/main/docs

## üìû Support et Contribution

### Probl√®me non r√©solu ?

1. ‚úÖ V√©rifiez les logs : `data/logs/scraping.log`
2. ‚úÖ Testez Ollama : `ollama run llama3.2 "test"`
3. ‚úÖ Testez ScrapingBee : v√©rifiez les cr√©dits
4. ‚úÖ Testez sur une seule URL simple d'abord

### Am√©liorations futures

- [ ] Support multi-threading
- [ ] Interface web (Flask/Streamlit)
- [ ] Export en JSON/Excel
- [ ] D√©tection automatique de la langue du r√©sum√©
- [ ] Cache des r√©sultats pour √©viter re-scraping
---