import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import os
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException

# ============================================
# VERSION AMÃ‰LIORÃ‰E - AVEC SELENIUM POUR JS
# ============================================(-t)

os.makedirs('output', exist_ok=True)

# ============================================
# CONFIGURATION SELENIUM
# ============================================

def create_driver():
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    try:
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    except Exception as e:
        print(f"âŒ Erreur crÃ©ation driver: {e}")
        print("ğŸ’¡ Installez ChromeDriver: pip install chromedriver-autoinstaller")
        return None

# ============================================
# FONCTIONS D'EXTRACTION AMÃ‰LIORÃ‰ES
# ============================================

def extract_text_from_url(url, use_selenium=False):
    js_sites = ['wahis.woah.org', 'app.', 'dashboard.', '#',
    'alyoum', 'aljazeera', 'akhbar', 'arab', 'saudi', 'gulf',
    'uae', 'kuwait', 'qatar', 'syria', 'iraq','elfagr', 
    '.sa', '.eg', '.qa', '.ae', '.ma', '.dz']
    needs_js = any(pattern in url for pattern in js_sites)
    
    if needs_js or use_selenium:
        return extract_with_selenium(url)
    else:
        return extract_with_requests(url)

def extract_with_requests(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        print(f"  ğŸ“¥ TÃ©lÃ©chargement (requests)...")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        if soup.find('h1'):
            titre = soup.find('h1').get_text(strip=True)
        elif soup.find('title'):
            titre = soup.find('title').get_text(strip=True)
        else:
            titre = "Sans titre"
        
        for script in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            script.decompose()
        
        text = soup.get_text(separator=' ', strip=True)
        text = re.sub(r'\s+', ' ', text).strip()
        
        print(f"  âœ… Texte extrait : {len(text)} caractÃ¨res")
        return titre, text, soup
        
    except Exception as e:
        print(f"  âŒ Erreur requests : {e}")
        return "", "", None

def extract_with_selenium(url):
    driver = None
    try:
        print(f"  ğŸŒ TÃ©lÃ©chargement (Selenium - JS)...")
        driver = create_driver()
        
        if not driver:
            print(f"  âš ï¸ Selenium non disponible, tentative avec requests...")
            return extract_with_requests(url)
        
        driver.get(url)
        
        wait = WebDriverWait(driver, 20)
        
        # ===========================
        # CORRECTION ICI âœ”âœ”âœ”
        # ===========================
        selectors_to_wait = [
            (By.TAG_NAME, 'h1'),
            (By.CLASS_NAME, 'article-body'),
            (By.CLASS_NAME, 'article-details'),
            (By.TAG_NAME, 'article'),
            (By.CLASS_NAME, 'content'),
            (By.ID, 'main'),
            (By.TAG_NAME, 'main')
        ]
        
        for by, selector in selectors_to_wait:
            try:
                wait.until(EC.presence_of_element_located((by, selector)))
                break
            except TimeoutException:
                continue
        
        time.sleep(3)
        
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        if soup.find('h1'):
            titre = soup.find('h1').get_text(strip=True)
        elif soup.find('title'):
            titre = soup.find('title').get_text(strip=True)
        else:
            titre = driver.title or "Sans titre"
        
        for script in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            script.decompose()

        # ===========================
        # CORRECTION EXTRACTION TEXTE âœ”âœ”âœ”
        # ===========================
        main_blocks = soup.find_all(
            ['article', 'div'],
            class_=['article-body', 'article-details', 'content']
        )

        if main_blocks:
            text = " ".join(block.get_text(" ", strip=True) for block in main_blocks)
        else:
            text = soup.get_text(separator=" ", strip=True)

        text = re.sub(r'\s+', ' ', text).strip()
        
        print(f"  âœ… Texte extrait : {len(text)} caractÃ¨res")
        
        return titre, text, soup
        
    except Exception as e:
        print(f"  âŒ Erreur Selenium : {e}")
        return "", "", None
    finally:
        if driver:
            driver.quit()

def detect_language(text):
    arabic_chars = re.findall(r'[\u0600-\u06FF]', text)
    
    if len(arabic_chars) > 5:
        return 'arabe'
    
    text_lower = text.lower()
    french_words = [' le ', ' la ', ' les ', ' des ', ' une ', ' dans ', ' pour ']
    english_words = [' the ', ' and ', ' of ', ' in ', ' to ', ' with ']

    fr_count = sum(w in text_lower for w in french_words)
    en_count = sum(w in text_lower for w in english_words)

    if fr_count > en_count:
        return 'franÃ§ais'
    elif en_count > fr_count:
        return 'anglais'
    else:
        return 'autre'

def extract_date(text, soup):
    date_patterns = [
        r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',
        r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',
        r'(\d{1,2})\s+(janvier|fÃ©vrier|mars|avril|mai|juin|juillet|aoÃ»t|septembre|octobre|novembre|dÃ©cembre)\s+(\d{4})',
        r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{4})',
    ]
    
    months_fr = {
        'janvier': '01', 'fÃ©vrier': '02', 'mars': '03', 'avril': '04',
        'mai': '05', 'juin': '06', 'juillet': '07', 'aoÃ»t': '08',
        'septembre': '09', 'octobre': '10', 'novembre': '11', 'dÃ©cembre': '12'
    }
    
    months_en = {
        'january': '01', 'february': '02', 'march': '03', 'april': '04',
        'may': '05', 'june': '06', 'july': '07', 'august': '08',
        'september': '09', 'october': '10', 'november': '11', 'december': '12'
    }
    
    for pattern in date_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            groups = match.groups()
            if len(groups) == 3:
                if groups[1].lower() in months_fr:
                    return f"{groups[0].zfill(2)}-{months_fr[groups[1].lower()]}-{groups[2]}"
                elif groups[1].lower() in months_en:
                    return f"{groups[0].zfill(2)}-{months_en[groups[1].lower()]}-{groups[2]}"
                elif groups[0].isdigit() and groups[1].isdigit():
                    return f"{groups[0].zfill(2)}-{groups[1].zfill(2)}-{groups[2]}"
    
    return "01-01-2025"

def extract_location(text):
    text_lower = text.lower()

    country_patterns = {
        "Tunisie": ["tunisie", "tunisia", "ØªÙˆÙ†Ø³"],
        "AlgÃ©rie": ["algÃ©rie", "algeria", "Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±"],
        "Maroc": ["maroc", "morocco", "Ø§Ù„Ù…ØºØ±Ø¨"],
        "Libye": ["libye", "libya", "Ù„ÙŠØ¨ÙŠØ§"],
        "Ã‰gypte": ["egypte", "egypt", "Ù…ØµØ±"],
        "Arabie Saoudite": ["saudi", "arabie saoudite", "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©", "Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©"],
        "Qatar": ["qatar", "Ù‚Ø·Ø±"],
        "Ã‰mirats Arabes Unis": ["emirates", "uae", "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª"],
        "BahreÃ¯n": ["bahrain", "bahrein", "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†"],
        "KoweÃ¯t": ["kuwait", "Ø§Ù„ÙƒÙˆÙŠØª"],
        "Bulgarie": ["bulgaria", "Ø¨Ù„ØºØ§Ø±ÙŠØ§"],
        "Jordanie": ["jordanie", "jordan", "Ø§Ù„Ø£Ø±Ø¯Ù†"],
        "Liban": ["liban", "lebanon", "Ù„Ø¨Ù†Ø§Ù†"],
        "Syrie": ["syrie", "syria", "Ø³ÙˆØ±ÙŠØ§"],
        "Turquie": ["turquie", "turkey", "ØªØ±ÙƒÙŠØ§"],
        "France": ["france"],
        "Italie": ["italie", "italy"],
        "Espagne": ["espagne", "spain"],
        "Allemagne": ["allemagne", "germany"],
        "Belgique": ["belgique", "belgium"],
        "Canada": ["canada"],
        "USA": ["usa", "united states", "Ã©tats-unis"],
        "Royaume-Uni": ["uk", "united kingdom", "royaume-uni"],
        "CorÃ©e du Sud": ["korea", "south korea", "ÙƒÙˆØ±ÙŠØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ©"],
    }

    city_patterns = {
        "Riyadh": ["riyadh", "Ø§Ù„Ø±ÙŠØ§Ø¶"],
        "Jeddah": ["jeddah", "Ø¬Ø¯Ø©"],
        "Sfax": ["sfax", "ØµÙØ§Ù‚Ø³"],
        "Tunis": ["tunis", "ØªÙˆÙ†Ø³"],
        "Kairouan": ["kairouan", "Ø§Ù„Ù‚ÙŠØ±ÙˆØ§Ù†"],
        "Casablanca": ["casablanca", "Ø§Ù„Ø¯Ø§Ø± Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡"],
        "Rabat": ["rabat", "Ø§Ù„Ø±Ø¨Ø§Ø·"],
        "Tripoli": ["tripoli", "Ø·Ø±Ø§Ø¨Ù„Ø³"],
        "Le Caire": ["cairo", "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©", "caire"],
        "East Boston": ["east boston"],
        "Damiette": ["damietta", "Ø¯Ù…ÙŠØ§Ø·"],
        "Sharqia": ["sharqia", "Ø§Ù„Ø´Ø±Ù‚ÙŠØ©"],
        "Zagazig": ["zagazig", "Ø§Ù„Ø²Ù‚Ø§Ø²ÙŠÙ‚"],
        "IsmaÃ¯lia": ["ismailia", "Ø§Ù„Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„ÙŠØ©"],
        "Port-SaÃ¯d": ["port said", "Ø¨ÙˆØ±Ø³Ø¹ÙŠØ¯"],
        "Suez": ["suez", "Ø§Ù„Ø³ÙˆÙŠØ³"],
        "Gizeh": ["giza", "Ø§Ù„Ø¬ÙŠØ²Ø©","Ù…Ø­Ø§ÙØ¸ Ø§Ù„Ø¬ÙŠØ²Ø©"],
        "Qalyubia": ["qalyubia", "Ø§Ù„Ù‚Ù„ÙŠÙˆØ¨ÙŠØ©"],
        "Assouan": ["aswan", "Ø£Ø³ÙˆØ§Ù†"],
        "Louxor": ["luxor", "Ø§Ù„Ø£Ù‚ØµØ±"],
        "Sohag": ["sohag", "Ø³ÙˆÙ‡Ø§Ø¬"],
        "Assiout": ["asyut", "Ø£Ø³ÙŠÙˆØ·"],
        "Minya": ["minya", "Ø§Ù„Ù…Ù†ÙŠØ§"],
        "Beheira": ["beheira", "Ø§Ù„Ø¨Ø­ÙŠØ±Ø©"],
        "Kafr el-Cheikh": ["kafr el-sheikh", "ÙƒÙØ± Ø§Ù„Ø´ÙŠØ®"],
        "Daqahliya": ["daqahliya", "Ø§Ù„Ø¯Ù‚Ù‡Ù„ÙŠØ©"],
        "Alexandrie": ["alexandria", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©"],
        "SinaÃ¯ du Nord": ["north sinai", "Ø´Ù…Ø§Ù„ Ø³ÙŠÙ†Ø§Ø¡"],
        "Cheikh Zuweid": ["sheikh zuweid", "Ø§Ù„Ø´ÙŠØ® Ø²ÙˆÙŠØ¯"],
        "Rafah": ["rafah", "Ø±ÙØ­"],
    }

    for country, patterns in country_patterns.items():
        for p in patterns:
            if p.lower() in text_lower:
                return country

    for city, patterns in city_patterns.items():
        for p in patterns:
            if p.lower() in text_lower:
                return city

    return "Non spÃ©cifiÃ©"

def extract_disease(text, langue):
    text_lower = text.lower()
    diseases = {
        "Anthrax": ["anthrax", "Ø§Ù„Ø¬Ù…Ø±Ø© Ø§Ù„Ø®Ø¨ÙŠØ«Ø©", "ç‚­ç–½"],
        "FiÃ¨vre de la VallÃ©e du Rift": ["rift valley fever", "rvf", "Ø­Ù…Ù‰ ÙˆØ§Ø¯ÙŠ Ø§Ù„Ù…ØªØµØ¯Ø¹", "Ø­Ù…Ù‰ ÙˆØ§Ø¯ÙŠ Ø§Ù„ØµØ¯Ø¹"],
        "FiÃ¨vre Catarrhale / Bluetongue": ["bluetongue", "blue tongue", "fiÃ¨vre catarrhale", "Ø§Ù„Ù„Ø³Ø§Ù† Ø§Ù„Ø£Ø²Ø±Ù‚"],
        "Brucellose (Brucella)": ["brucella", "brucellose", "brucellosis", "Ø§Ù„Ø¨Ø±ÙˆØ³ÙŠÙ„Ø§", "Ø­Ù…Ù‰ Ù…Ø§Ù„Ø·ÙŠØ©"],
        "Grippe Ã©quine (Equine Influenza)": [
            "equine influenza", "grippe Ã©quine", "Ø§Ù†ÙÙ„ÙˆÙ†Ø²Ø§ Ø§Ù„Ø®ÙŠÙˆÙ„", "influenza equina"
        ],
        "SARS-CoV-2 / COVID-19": [
            "sars-cov-2", "covid-19", "covid19", "coronavirus", "ÙÙŠØ±ÙˆØ³ ÙƒÙˆØ±ÙˆÙ†Ø§", "ÙƒÙˆÙÙŠØ¯-19",
            "covid chez les animaux", "sars cov 2 chez les animaux"
        ],
        "Rage": ["rabies", "rage", "Ø¯Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø¨"],
        "FiÃ¨vre Aphteuse": ["foot and mouth disease", "fmd", "fiÃ¨vre aphteuse", "Ø§Ù„Ø­Ù…Ù‰ Ø§Ù„Ù‚Ù„Ø§Ø¹ÙŠØ©"],
        "Maladie de Newcastle": ["newcastle disease", "newcastle", "Ù…Ø±Ø¶ Ù†ÙŠÙˆÙƒØ§Ø³Ù„", "Ù†ÙŠÙˆÙƒØ§Ø³Ù„"],
        "Maladie d'Aujeszky": ["aujeszky", "pseudorabies", "Ù…Ø±Ø¶ Ø£ÙˆÙŠØ²ÙƒÙŠ", "Ø£ÙˆÙŠØ²ÙƒÙŠ"],
        "Heartwater": ["heartwater", "ehrlichia ruminantium", "Ø¥ÙŠØ±Ù„ÙŠØ´ÙŠØ§"],
        "EHD / Maladie hÃ©morragique Ã©pizootique": [
            "epizootic hemorrhagic disease", "epizootic haemorrhagic disease",
            "maladie hÃ©morragique Ã©pizootique", "maladie hemorragique epizootique",
            "hÃ©morragique Ã©pizootique", "hemorragique epizootique",
            "hÃ©morragique Ã©pidÃ©mique", "hemorragique epidemique",
            "maladie hÃ©morragique Ã©pidÃ©mique",
            "Ù…Ø±Ø¶ Ø§Ù„Ù†Ø²Ù Ø§Ù„ÙˆØ¨Ø§Ø¦ÙŠ", "Ù…Ø±Ø¶ Ù†Ø²ÙŠÙ ÙˆØ¨Ø§Ø¦ÙŠ", "Ø§Ù„Ù…Ø±Ø¶ Ø§Ù„Ù†Ø²ÙÙŠ Ø§Ù„ÙˆØ¨Ø§Ø¦ÙŠ"
        ],
        "FiÃ¨vre de West Nile": ["fiÃ¨vre de west nile", "west nile fever", "Ø­Ù…Ù‰ ØºØ±Ø¨ Ø§Ù„Ù†ÙŠÙ„"],
        "Dermatose Nodulaire Contagieuse (LSD)": [
            "lumpy skin disease", "lsd", "Ø§Ù„Ø¬Ù„Ø¯ Ø§Ù„Ø¹Ù‚Ø¯ÙŠ",
            "dermatose nodulaire contagieuse (inf. par le virus de la)",
            "dermatose nodulaire contagieuse", "dermatose nodulaire"
        ],
        "Tuberculose": ["tuberculose", "tuberculosis", "Ø§Ù„Ø³Ù„"],
        "Trypanosomose (Surra)": ["trypanosoma evansi", "surra", "ØªØ±ÙŠØ¨Ø§Ù†ÙˆØ³ÙˆÙ…Ø§", "Ø³ÙˆØ±Ø§"],
        "Tularemia": ["tularemia", "tularemie", "ØªØ§Ù„Ø§Ø±ÙŠÙ…ÙŠØ§"],
        "Anaplasmose bovine": ["anaplasmosis", "anaplasmose", "Ø£Ù†Ø§Ø¨Ù„Ø§Ø²Ù…Ø§"],
        "BabÃ©siose": ["babesiosis", "babÃ©siose", "Ø¨Ø§Ø¨ÙŠØ²ÙŠØ§"],
        "NÃ©crose hÃ©matopoÃ¯Ã©tique infectieuse": [
            "nÃ©crose hÃ©matopoÃ¯Ã©tique infectieuse", "infectious hematopoietic necrosis",
            "Ù…Ø±Ø¶ Ø§Ù„Ù†Ø®Ø± Ø§Ù„Ø¯Ù…ÙˆÙŠ Ø§Ù„Ù…Ø¹Ø¯ÙŠ"
        ],
        "Ã‰chinococcose / Hydatidose": ["echinococcus", "hydatidose", "echinococcosis", "Ø¥Ø´ÙŠÙ†ÙˆÙƒÙˆÙƒØ³"],
        "Peste des Petits Ruminants": ["peste des petits ruminants", "ppr", "Ø·Ø§Ø¹ÙˆÙ† Ø§Ù„Ù…Ø¬ØªØ±Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©"],
        "Peste Porcine Africaine": ["african swine fever", "asf", "Ø§Ù„Ø­Ù…Ù‰ Ø§Ù„Ø£ÙØ±ÙŠÙ‚ÙŠØ© Ù„Ù„Ø®Ù†Ø§Ø²ÙŠØ±"],
        "Peste Porcine Classique": ["classical swine fever", "csf", "Ø­Ù…Ù‰ Ø§Ù„Ø®Ù†Ø§Ø²ÙŠØ± Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©"],
        "Peste Ã‰quine": ["equine plague", "Ø·Ø§Ø¹ÙˆÙ† Ø§Ù„Ø®ÙŠÙ„"],
        "Peste":["plague", "peste", "Ø§Ù„Ø·Ø§Ø¹ÙˆÙ†"," Ù…Ø±Ø¶ Ø·Ø§Ø¹ÙˆÙ†"],
        "Peste Aviaire (Influenza Aviaire)": [
            "avian influenza", "influenza aviaire", "bird flu", "Ø§Ù†ÙÙ„ÙˆÙ†Ø²Ø§ Ø§Ù„Ø·ÙŠÙˆØ±", "Ø¥Ù†ÙÙ„ÙˆÙ†Ø²Ø§ Ø§Ù„Ø·Ù‘ÙŠÙˆØ±"
        ],
        "FiÃ¨vre HÃ©morragique CrimÃ©e-Congo": [
            "crimean congo hemorrhagic fever", "cchf", "Ø­Ù…Ù‰ Ø§Ù„Ù‚Ø±Ù… Ø§Ù„ÙƒÙˆÙ†ØºÙˆ Ø§Ù„Ù†Ø²ÙÙŠØ©"
        ],
        "Rinderpest": ["rinderpest", "peste bovine", "Ø·Ø§Ø¹ÙˆÙ† Ø§Ù„Ø£Ø¨Ù‚Ø§Ø±"],
        "Paratuberculose": ["paratuberculosis", "paratuberculose", "Ø¨Ø§Ø±Ø§ØªÙˆØ¨Ø±ÙƒÙˆÙ„ÙˆØ²"],
        "ClavelÃ©e et variole caprine": [
            "clavelÃ©e", "clavele", "variole caprine", "variole ovine",
            "sheep pox", "goat pox", "sheeppox", "goatpox",
            "Ø¬Ø¯Ø±ÙŠ Ø§Ù„Ø£ØºÙ†Ø§Ù…", "Ø¬Ø¯Ø±ÙŠ Ø§Ù„Ù…Ø§Ø¹Ø²", "Ø§Ù„Ø¬Ø¯Ø±ÙŠ"
        ],
        "Aethina tumida": [
            "aethina tumida", "petit colÃ©optÃ¨re des ruches", "petit colÃ©optÃ¨re de la ruche",
            "small hive beetle", "shb", "Ø®Ù†ÙØ³Ø§Ø¡ Ø§Ù„Ø®Ù„ÙŠØ© Ø§Ù„ØµØºÙŠØ±Ø©"
        ]
    }
    
    for disease, keywords in diseases.items():
        for kw in keywords:
            if kw.lower() in text_lower:
                return disease
    
    return "Non identifiÃ©e"

def detect_source_type(url, text):
    official_domains = ['gouv', 'gov', 'who', 'woah', 'oie', 'fao', 'europa.eu', 'cdc']
    media_keywords = ['journal', 'news', 'presse', 'radio', 'tv', 'mÃ©dia', 'media']
    social_domains = ['facebook', 'twitter', 'instagram', 'linkedin']
    
    url_lower = url.lower()
    text_lower = text.lower()
    
    for domain in official_domains:
        if domain in url_lower:
            return "site officiel"
    
    for domain in social_domains:
        if domain in url_lower:
            return "rÃ©seaux sociaux"
    
    for keyword in media_keywords:
        if keyword in text_lower[:500]:
            return "mÃ©dias"
    
    return "mÃ©dias"

def create_summary(text, word_count):
    words = text.split()
    if len(words) <= word_count:
        return text
    
    summary_words = words[:word_count]
    summary = ' '.join(summary_words)
    
    last_period = summary.rfind('.')
    if last_period > 0:
        summary = summary[:last_period + 1]
    
    return summary

def extract_named_entities(text):
    entities = []
    
    orgs = ['OMS', 'WHO', 'FAO', 'OIE', 'WOAH', 'ministÃ¨re', 'Ministry', 'APQA']
    for org in orgs:
        if org.lower() in text.lower():
            entities.append(org)
    
    animals = ['bovins', 'volailles', 'porcs', 'poulets', 'cattle', 'poultry', 'pigs', 'chickens']
    for animal in animals:
        if animal.lower() in text.lower():
            entities.append(animal)
    
    return list(set(entities))

# ============================================
# TRAITEMENT PRINCIPAL
# ============================================

def main():
    print("=" * 70)
    print("ğŸ¦  EXTRACTION DES NEWS SUR LES MALADIES ANIMALES")
    print("ğŸš€ VERSION AMÃ‰LIORÃ‰E - AVEC SUPPORT JAVASCRIPT (SELENIUM)")
    print("=" * 70)
    
    print("\nğŸ“‚ Chargement des URLs...")
    df_urls = pd.read_csv('urls.csv')
    print(f"âœ… {len(df_urls)} URLs chargÃ©es")
    
    print("\nğŸ§ª Test de Selenium...")
    test_driver = create_driver()
    if test_driver:
        print("âœ… Selenium opÃ©rationnel")
        test_driver.quit()
    else:
        print("âš ï¸ Selenium non disponible - utilisation de requests uniquement")
    
    results = []
    
    for idx, row in df_urls.head(50).iterrows():
        code = row['code']
        url = row['url']
        
        print(f"\n{'='*70}")
        print(f"ğŸ“„ [{idx+1}/50] Traitement de {code}")
        print(f"{'='*70}")
        print(f"ğŸ”— {url}")
        
        titre, text, soup = extract_text_from_url(url)
        
        if not text or len(text) < 100:
            print(f"  âš ï¸ Contenu insuffisant ({len(text)} caractÃ¨res), passage au suivant")
            
            results.append({
                'code': code,
                'url': url,
                'titre': 'ERREUR: Contenu insuffisant',
                'contenu': text,
                'langue': 'N/A',
                'nb_caracteres': len(text),
                'nb_mots': 0,
                'date_publication': 'N/A',
                'lieu': 'N/A',
                'maladie': 'N/A',
                'source_type': 'N/A',
                'resume_50': '',
                'resume_100': '',
                'resume_150': '',
                'entites_nommees': ''
            })
            continue
        
        langue = detect_language(text)
        print(f"  ğŸŒ Langue : {langue}")
        
        nb_mots = len(text.split())
        nb_caracteres = len(text)
        print(f"  ğŸ“Š {nb_mots} mots, {nb_caracteres} caractÃ¨res")
        
        date_pub = extract_date(text, soup)
        print(f"  ğŸ“… Date : {date_pub}")
        
        lieu = extract_location(text)
        print(f"  ğŸ“ Lieu : {lieu}")
        
        maladie = extract_disease(text, langue)
        print(f"  ğŸ¦  Maladie : {maladie}")
        
        source_type = detect_source_type(url, text)
        print(f"  ğŸ“° Source : {source_type}")
        
        resume_50 = create_summary(text, 50)
        resume_100 = create_summary(text, 100)
        resume_150 = create_summary(text, 150)
        
        entites = extract_named_entities(text)
        
        result = {
            'code': code,
            'url': url,
            'titre': titre,
            'contenu': text,
            'langue': langue,
            'nb_caracteres': nb_caracteres,
            'nb_mots': nb_mots,
            'date_publication': date_pub,
            'lieu': lieu,
            'maladie': maladie,
            'source_type': source_type,
            'resume_50': resume_50,
            'resume_100': resume_100,
            'resume_150': resume_150,
            'entites_nommees': ';'.join(entites)
        }
        
        results.append(result)
        print(f"  âœ… DonnÃ©es enregistrÃ©es")
        
        if len(results) % 5 == 0:
            df_temp = pd.DataFrame(results)
            df_temp.to_csv('output/animal_diseases_dataset.csv', index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ Sauvegarde temporaire : {len(results)} entrÃ©es")
        
        time.sleep(1)
    
    print("\n" + "=" * 70)
    print("ğŸ’¾ SAUVEGARDE DU DATASET FINAL")
    print("=" * 70)
    
    df_final = pd.DataFrame(results)
    output_file = 'output/animal_diseases_dataset.csv'
    df_final.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… Dataset sauvegardÃ© : {output_file}")
    print(f"ğŸ“Š Total d'entrÃ©es : {len(df_final)}")
    
    print("\n" + "=" * 70)
    print("ğŸ“ˆ STATISTIQUES DU DATASET")
    print("=" * 70)
    
    df_valid = df_final[df_final['langue'] != 'N/A']
    
    print(f"\nâœ… EntrÃ©es valides : {len(df_valid)} / {len(df_final)}")
    print(f"âŒ EntrÃ©es en erreur : {len(df_final) - len(df_valid)}")
    
    if len(df_valid) > 0:
        print(f"\nğŸŒ RÃ©partition par langue :")
        print(df_valid['langue'].value_counts())
        
        print(f"\nğŸ“° RÃ©partition par type de source :")
        print(df_valid['source_type'].value_counts())
        
        print(f"\nğŸ¦  Top 10 des maladies :")
        print(df_valid['maladie'].value_counts().head(10))
        
        print(f"\nğŸ“ Top 10 des lieux :")
        print(df_valid['lieu'].value_counts().head(10))
        
        print(f"\nğŸ“ Statistiques sur les mots :")
        print(f"  Moyenne : {df_valid['nb_mots'].mean():.0f}")
        print(f"  MÃ©diane : {df_valid['nb_mots'].median():.0f}")
        print(f"  Min     : {df_valid['nb_mots'].min()}")
        print(f"  Max     : {df_valid['nb_mots'].max()}")
    
    print("\n" + "=" * 70)
    print("âœ¨ TRAITEMENT TERMINÃ‰ AVEC SUCCÃˆS !")
    print("=" * 70)

if __name__ == "__main__":
    main()
