import pandas as pd
from src.scraper import setup_driver, extract_article_data
from src.utils import clean_text, detect_language, get_domain_type
from src.llm_processor import extract_fields_with_llm
import logging
import time
import sys

INPUT_FILE = "data/input/urls.csv"
OUTPUT_FILE = "data/output/output_dataset.csv"

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("data/logs/scraping.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

def load_csv_auto_detect():
    """Charge le CSV en d√©tectant automatiquement le d√©limiteur"""
    delimiters = [',', ';', '\t', '|', ' ']
    
    for delimiter in delimiters:
        try:
            df = pd.read_csv(INPUT_FILE, sep=delimiter, encoding='utf-8')
            
            # V√©rifier que le fichier a bien au moins 2 colonnes
            if len(df.columns) >= 2:
                logging.info(f"‚úì D√©limiteur d√©tect√© : '{delimiter}'")
                logging.info(f"  Colonnes trouv√©es : {df.columns.tolist()}")
                return df
        except:
            continue
    
    # Si aucun d√©limiteur ne fonctionne, essayer sans sp√©cifier
    try:
        df = pd.read_csv(INPUT_FILE, encoding='utf-8')
        if len(df.columns) >= 2:
            return df
    except:
        pass
    
    raise ValueError("Impossible de lire le fichier CSV avec les d√©limiteurs standards")

def detect_columns(df):
    """D√©tecte automatiquement les colonnes ID et URL"""
    columns = df.columns.tolist()
    
    # Si une seule colonne qui contient les deux infos, essayer de s√©parer
    if len(columns) == 1:
        col_name = columns[0]
        # V√©rifier si la colonne contient deux valeurs s√©par√©es
        first_val = str(df[col_name].iloc[0])
        if ' ' in first_val or '\t' in first_val:
            logging.warning("‚ö†Ô∏è  D√©tection d'une seule colonne avec valeurs s√©par√©es")
            logging.warning("    Tentative de s√©paration automatique...")
            
            # Essayer de s√©parer
            for sep in [' ', '\t', ';', ',']:
                if sep in first_val:
                    df[['id_temp', 'url_temp']] = df[col_name].str.split(sep, n=1, expand=True)
                    df = df[['id_temp', 'url_temp']]
                    df.columns = ['code', 'lien']
                    logging.info(f"    ‚úì S√©paration effectu√©e avec '{sep}'")
                    return 'code', 'lien', df
    
    # D√©tecter la colonne URL
    url_col = None
    for col in columns:
        if any(keyword in col.lower() for keyword in ['url', 'lien', 'link', 'site', 'web']):
            url_col = col
            break
    
    # D√©tecter la colonne ID/Code
    id_col = None
    for col in columns:
        if any(keyword in col.lower() for keyword in ['code', 'id', 'identifiant', 'reference', 'ref']):
            id_col = col
            break
    
    # Par d√©faut : premi√®res colonnes
    if not id_col and len(columns) >= 1:
        id_col = columns[0]
    if not url_col and len(columns) >= 2:
        url_col = columns[1]
    
    return id_col, url_col, df

def main():
    logging.info("üöÄ D√©marrage du scraping...")
    
    # Charger le fichier d'entr√©e avec auto-d√©tection
    try:
        df_input = load_csv_auto_detect()
        logging.info(f"‚úì Fichier charg√© : {len(df_input)} URLs √† traiter")
    except FileNotFoundError:
        logging.error(f"‚ùå Fichier introuvable : {INPUT_FILE}")
        return
    except Exception as e:
        logging.error(f"‚ùå Erreur de lecture du fichier : {e}")
        logging.error("üí° V√©rifiez que votre CSV a bien 2 colonnes s√©par√©es par , ou ;")
        return
    
    # D√©tecter les colonnes
    result = detect_columns(df_input)
    if len(result) == 3:
        id_col, url_col, df_input = result
    else:
        id_col, url_col = result[0], result[1]
    
    if not id_col or not url_col:
        logging.error("‚ùå Impossible de d√©tecter les colonnes ID et URL")
        logging.error(f"Colonnes disponibles : {df_input.columns.tolist()}")
        return
    
    logging.info(f"‚úì Colonnes utilis√©es - ID: '{id_col}', URL: '{url_col}'")
    
    # V√©rifier que les colonnes existent
    if id_col not in df_input.columns or url_col not in df_input.columns:
        logging.error(f"‚ùå Colonnes manquantes dans le fichier")
        return
    
    # Configuration du driver
    try:
        driver = setup_driver()
        logging.info("‚úì Driver Selenium initialis√©")
    except Exception as e:
        logging.error(f"‚ùå Erreur d'initialisation du driver : {e}")
        return

    results = []
    total = len(df_input)
    errors = 0

    for idx, row in df_input.iterrows():
        try:
            code = str(row[id_col]).strip()
            url = str(row[url_col]).strip()
            
            # V√©rifier que l'URL est valide
            if not url.startswith('http'):
                logging.warning(f"‚ö†Ô∏è  URL invalide pour {code}: {url}")
                errors += 1
                continue
            
            logging.info(f"[{idx+1}/{total}] Traitement [{code}] : {url}")

            # Phase 1 : Scraping
            raw_data = extract_article_data(driver, url)
            contenu_clean = clean_text(raw_data["contenu"])
            
            # V√©rifier que le contenu n'est pas vide
            if not contenu_clean or len(contenu_clean) < 50:
                logging.warning(f"‚ö†Ô∏è  Contenu trop court pour {code} ({len(contenu_clean)} caract√®res)")
                
                # Si vraiment vide, utiliser le contenu brut
                if len(contenu_clean) < 10:
                    contenu_clean = raw_data["contenu"]
                
                # Si toujours vide, skipper
                if len(contenu_clean) < 20:
                    logging.error(f"‚ùå Impossible d'extraire du contenu pour {code}, skip")
                    errors += 1
                    continue
            
            langue = detect_language(contenu_clean)
            source_type = get_domain_type(url)

            # Phase 2 : LLM
            logging.info(f"  ‚Üí Extraction LLM en cours...")
            llm_fields = extract_fields_with_llm(contenu_clean, url)

            # Compter caract√®res et mots
            nb_caracteres = len(contenu_clean)
            nb_mots = len(contenu_clean.split())

            # Construire la ligne finale
            final_row = {
                "code": code,
                "url": url,
                "titre": raw_data["titre"],
                "contenu": contenu_clean,
                "langue": langue,
                "nb_caracteres": nb_caracteres,
                "nb_mots": nb_mots,
                "date_publication": llm_fields.get("date_publication", "inconnue"),
                "lieu": llm_fields.get("lieu", "inconnu"),
                "maladie": llm_fields.get("maladie", "inconnue"),
                "animal": llm_fields.get("animal", "inconnu"),
                "source_publication": source_type,
                "resume_50_mots": llm_fields.get("resume_50_mots", "R√©sum√© indisponible."),
                "resume_100_mots": llm_fields.get("resume_100_mots", "R√©sum√© indisponible."),
                "resume_150_mots": llm_fields.get("resume_150_mots", "R√©sum√© indisponible.")
            }

            results.append(final_row)
            logging.info(f"  ‚úì Trait√© avec succ√®s ({nb_mots} mots, langue: {langue})")

            # Sauvegarde partielle tous les 5 √©l√©ments
            if (idx + 1) % 5 == 0:
                pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False, encoding='utf-8')
                logging.info(f"  üíæ Sauvegarde interm√©diaire ({len(results)} r√©sultats)")

            time.sleep(1)  # √ätre gentil avec les serveurs

        except KeyboardInterrupt:
            logging.warning("\n‚ö†Ô∏è  Interruption par l'utilisateur")
            break
        except Exception as e:
            logging.error(f"‚ùå Erreur pour la ligne {idx+1} ({code}) : {str(e)}")
            errors += 1
            # Continuer avec l'URL suivante
            continue

    # Fermeture propre
    try:
        driver.quit()
        logging.info("‚úì Driver ferm√©")
    except:
        pass
    
    # Sauvegarde finale
    if results:
        pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False, encoding='utf-8')
    
    # R√©sum√© final
    logging.info("="*60)
    logging.info(f"‚úÖ Scraping termin√© :")
    logging.info(f"   ‚Ä¢ Succ√®s : {len(results)}/{total} URLs")
    logging.info(f"   ‚Ä¢ Erreurs : {errors}")
    logging.info(f"üìÅ R√©sultats sauvegard√©s dans : {OUTPUT_FILE}")
    logging.info("="*60)

if __name__ == "__main__":
    main()